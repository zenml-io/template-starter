{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d36bb-a162-4b8e-959d-30aa0f514472",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml init\n",
    "!zenml stack set local-sagemaker-step-operator-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53367f1-3951-48c7-9540-21daf818fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the imports at the top\n",
    "\n",
    "import random\n",
    "from zenml import ExternalArtifact, pipeline \n",
    "from zenml.client import Client\n",
    "from zenml.logger import get_logger\n",
    "from uuid import UUID\n",
    "\n",
    "import os\n",
    "from typing import Optional, List\n",
    "\n",
    "from zenml import pipeline\n",
    "\n",
    "from steps import (\n",
    "    data_loader,\n",
    "    data_preprocessor,\n",
    "    data_splitter,\n",
    "    model_evaluator,\n",
    "    model_trainer,\n",
    "    inference_predict,\n",
    "    inference_preprocessor\n",
    ")\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4bad40-455d-416b-b5f7-e895a2561ca2",
   "metadata": {},
   "source": [
    "# Run the feature engineering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609a742-0b10-4c7f-8cb0-02a6f74b368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def feature_engineering(\n",
    "    test_size: float = 0.2,\n",
    "    drop_na: Optional[bool] = None,\n",
    "    normalize: Optional[bool] = None,\n",
    "    drop_columns: Optional[List[str]] = None,\n",
    "    target: Optional[str] = \"target\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Feature engineering pipeline.\n",
    "\n",
    "    This is a pipeline that loads the data, processes it and splits\n",
    "    it into train and test sets.\n",
    "\n",
    "    Args:\n",
    "        test_size: Size of holdout set for training 0.0..1.0\n",
    "        drop_na: If `True` NA values will be removed from dataset\n",
    "        normalize: If `True` dataset will be normalized with MinMaxScaler\n",
    "        drop_columns: List of columns to drop from dataset\n",
    "        target: Name of target column in dataset\n",
    "    \"\"\"\n",
    "    ### ADD YOUR OWN CODE HERE - THIS IS JUST AN EXAMPLE ###\n",
    "    # Link all the steps together by calling them and passing the output\n",
    "    # of one step as the input of the next step.\n",
    "    raw_data = data_loader(random_state=random.randint(0, 100), target=target)\n",
    "    dataset_trn, dataset_tst = data_splitter(\n",
    "        dataset=raw_data,\n",
    "        test_size=test_size,\n",
    "    )\n",
    "    dataset_trn, dataset_tst, _ = data_preprocessor(\n",
    "        dataset_trn=dataset_trn,\n",
    "        dataset_tst=dataset_tst,\n",
    "        drop_na=drop_na,\n",
    "        normalize=normalize,\n",
    "        drop_columns=drop_columns,\n",
    "        target=target,\n",
    "    )\n",
    "    \n",
    "    return dataset_trn, dataset_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1451bc-d255-40cc-9e90-3d45fd07a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_args = {}\n",
    "pipeline_args[\"config_path\"] = os.path.join(\"configs\", \"feature_engineering.yaml\")\n",
    "fe_p_configured = feature_engineering.with_options(**pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a7407-8cc9-41f4-a80f-8b711e57b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_run = fe_p_configured()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87746e-b804-4fab-88f6-d4967048cb45",
   "metadata": {},
   "source": [
    "# Run the training Pipeline\n",
    "\n",
    "![title](_assets/default_stack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa300f1-48df-4e62-87eb-0e2fc5735da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def training(\n",
    "    train_dataset_id: Optional[UUID] = None,\n",
    "    test_dataset_id: Optional[UUID] = None,\n",
    "    min_train_accuracy: float = 0.0,\n",
    "    min_test_accuracy: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Model training pipeline.\n",
    "\n",
    "    This is a pipeline that loads the data, processes it and splits\n",
    "    it into train and test sets, then search for best hyperparameters,\n",
    "    trains and evaluates a model.\n",
    "\n",
    "    Args:\n",
    "        test_size: Size of holdout set for training 0.0..1.0\n",
    "        drop_na: If `True` NA values will be removed from dataset\n",
    "        normalize: If `True` dataset will be normalized with MinMaxScaler\n",
    "        drop_columns: List of columns to drop from dataset\n",
    "    \"\"\"\n",
    "    ### ADD YOUR OWN CODE HERE - THIS IS JUST AN EXAMPLE ###\n",
    "    # Link all the steps together by calling them and passing the output\n",
    "    # of one step as the input of the next step.\n",
    "    \n",
    "    # Execute Feature Engineering Pipeline\n",
    "    if train_dataset_id is None or test_dataset_id is None:\n",
    "        dataset_trn, dataset_tst = feature_engineering()\n",
    "    else:\n",
    "        dataset_trn = ExternalArtifact(id=train_dataset_id)\n",
    "        dataset_tst = ExternalArtifact(id=test_dataset_id)\n",
    "    \n",
    "    model = model_trainer(\n",
    "        dataset_trn=dataset_trn,\n",
    "    )\n",
    "\n",
    "    model_evaluator(\n",
    "        model=model,\n",
    "        dataset_trn=dataset_trn,\n",
    "        dataset_tst=dataset_tst,\n",
    "        min_train_accuracy=min_train_accuracy,\n",
    "        min_test_accuracy=min_test_accuracy,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55342bf-33c5-4646-b1ce-e599a99cf568",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_args = {}\n",
    "pipeline_args[\"config_path\"] = os.path.join(\"configs\", \"training.yaml\")\n",
    "fe_t_configured = training.with_options(**pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4aed8-7d87-4e07-a25c-345d327ad636",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_t_configured()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f40bd2e-14fb-4989-9545-a577a3be479a",
   "metadata": {},
   "source": [
    "# Switch the Stack\n",
    "\n",
    "![title](_assets/airflow_stack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dd948-6a55-466e-b711-c919eed7cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack set sagemaker-airflow-stack\n",
    "!zenml integration install airflow aws\n",
    "!pip install apache-airflow-providers-docker apache-airflow~=2.5.0\n",
    "!zenml stack up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638122d6-d282-4ce7-a040-5ed429750c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_args = {}\n",
    "step_args[\"step_operator\"] = \"sagemaker-eu\"\n",
    "step_args[\"settings\"] = {\"step_operator.sagemaker\": {\"estimator_args\": {\"instance_type\" : \"ml.m5.large\"}}}\n",
    "\n",
    "model_trainer_configured = model_trainer.with_options(**step_args)\n",
    "\n",
    "@pipeline\n",
    "def training(\n",
    "    train_dataset_id: Optional[UUID] = None,\n",
    "    test_dataset_id: Optional[UUID] = None,\n",
    "    min_train_accuracy: float = 0.0,\n",
    "    min_test_accuracy: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Model training pipeline.\n",
    "\n",
    "    This is a pipeline that loads the data, processes it and splits\n",
    "    it into train and test sets, then search for best hyperparameters,\n",
    "    trains and evaluates a model.\n",
    "\n",
    "    Args:\n",
    "        test_size: Size of holdout set for training 0.0..1.0\n",
    "        drop_na: If `True` NA values will be removed from dataset\n",
    "        normalize: If `True` dataset will be normalized with MinMaxScaler\n",
    "        drop_columns: List of columns to drop from dataset\n",
    "    \"\"\"\n",
    "    ### ADD YOUR OWN CODE HERE - THIS IS JUST AN EXAMPLE ###\n",
    "    # Link all the steps together by calling them and passing the output\n",
    "    # of one step as the input of the next step.\n",
    "    \n",
    "    # Execute Feature Engineering Pipeline\n",
    "    if train_dataset_id is None or test_dataset_id is None:\n",
    "        dataset_trn, dataset_tst = feature_engineering()\n",
    "    else:\n",
    "        dataset_trn = ExternalArtifact(id=train_dataset_id)\n",
    "        dataset_tst = ExternalArtifact(id=test_dataset_id)\n",
    "    \n",
    "    model = model_trainer_configured(\n",
    "        dataset_trn=dataset_trn,\n",
    "    )\n",
    "\n",
    "    model_evaluator(\n",
    "        model=model,\n",
    "        dataset_trn=dataset_trn,\n",
    "        dataset_tst=dataset_tst,\n",
    "        min_train_accuracy=min_train_accuracy,\n",
    "        min_test_accuracy=min_test_accuracy,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206a48b-3a2a-4dc3-af42-a5f6ba9ccd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_args = {}\n",
    "pipeline_args[\"config_path\"] = os.path.join(\"configs\", \"training.yaml\")\n",
    "fe_t_configured = training.with_options(**pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188c8c5-3e8f-42b2-9f98-380c265cf8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_t_configured()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0841f93b-9eb5-4af6-bba7-cec167024ccf",
   "metadata": {},
   "source": [
    "# Switch to full Sagemaker Stack\n",
    "\n",
    "![title](_assets/sagemaker_stack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e33484-3377-4f0e-83fa-87d7c0ca4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack set sagemaker-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c95e9-df2e-446c-8d61-9cc37ad8a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_t_configured()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e46721-3733-439e-a03e-54512552eed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
